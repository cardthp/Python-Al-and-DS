{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I would like to change a requirements from write data to PostgresSQL to save in csv format on localhost and explain it start at row108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lib\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "\n",
    "#for Create SparkSeesion on Localhost\n",
    "#spark: SparkSession = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "#for Create SparkSeesion via docker-compose\n",
    "spark: SparkSession = SparkSession.builder.master('spark://spark:7077').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step1 : Get data transaction file to Spark dataframe\n",
    "df = spark.read.csv('data/transaction.csv', sep='|', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+---------------+-----------+---------+\n",
      "|   transactionId|custId|transactionDate|productSold|unitsSold|\n",
      "+----------------+------+---------------+-----------+---------+\n",
      "|20120302-0023938| 23938|     2012-03-02|    PURA250|       54|\n",
      "|20120302-0023938| 23938|     2012-03-02|    SUPA102|       42|\n",
      "|20120302-0023938| 23938|     2012-03-02|    SUPA105|       39|\n",
      "|20120315-0023938| 23938|     2012-03-15|    PURA250|       57|\n",
      "|20120724-0023938| 23938|     2012-07-24|    DETA800|       49|\n",
      "|20120725-0023938| 23938|     2012-07-25|    PURA500|       52|\n",
      "|20120725-0023938| 23938|     2012-07-25|    SUPA104|       48|\n",
      "|20120903-0023938| 23938|     2012-09-03|    PURA100|       51|\n",
      "+----------------+------+---------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Select customerID before beacuse it will reduce runtime to query the next process \n",
    "or if you don't want to hardcode on customerID it can use parameter that get input data via keyboard by create variable\n",
    "\"\"\"\n",
    "df_SelectCustomer = df.where(df['custId'] == '0023938')\n",
    "\n",
    "#Create view that filtered a customerID\n",
    "df_SelectCustomer.createOrReplaceTempView('transaction_SelectCustomer')\n",
    "\n",
    "df_SelectCustomer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|custId|DateDiff_InDays|\n",
      "+------+---------------+\n",
      "| 23938|            185|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step2 : Query view that filter a customerID to find longest_streak by maxdate minus mindate\n",
    "df_longest_streak = spark.sql(\"\"\"\n",
    "SELECT custId,\n",
    "DATEDIFF(to_date(max(transactionDate), 'yyyy-MM-dd'),to_date(min(transactionDate), 'yyyy-MM-dd')) AS DateDiff_InDays\n",
    "FROM transaction_SelectCustomer\n",
    "GROUP BY custId\n",
    "\"\"\")\n",
    "\n",
    "#Create view that show longest_streak \n",
    "df_longest_streak.createOrReplaceTempView('view_longest_streak')\n",
    "\n",
    "df_longest_streak.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+\n",
      "|custId|productSold|unitsSold|\n",
      "+------+-----------+---------+\n",
      "| 23938|    PURA250|      111|\n",
      "+------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step3 : Query view that filter a customerID to find favourite_product it have 3 parts\n",
    "#For part1 to find favourite_product it need to find unitsSold from each productSold\n",
    "df_favourite_product_sumUnitsSold = spark.sql(\"\"\"\n",
    "SELECT custId,\n",
    "productSold,\n",
    "SUM(unitsSold) AS unitsSold\n",
    "FROM transaction_SelectCustomer\n",
    "GROUP BY custId,productSold\n",
    "\"\"\")\n",
    "\n",
    "df_favourite_product_sumUnitsSold.createOrReplaceTempView('view_favourite_product_sumUnitsSold')\n",
    "\n",
    "#For part2 to find favourite_product it need to descending unitsSold by create Row_Number\n",
    "df_favourite_product_sumUnitsSold_DESC = spark.sql(\"\"\"\n",
    "SELECT custId,\n",
    "productSold,\n",
    "unitsSold,\n",
    "ROW_NUMBER() OVER (PARTITION BY custId ORDER BY unitsSold DESC) AS row_num\n",
    "FROM view_favourite_product_sumUnitsSold\n",
    "\"\"\")\n",
    "\n",
    "df_favourite_product_sumUnitsSold_DESC.createOrReplaceTempView('view_favourite_product_sumUnitsSold_DESC')\n",
    "\n",
    "#For part3 to find favourite_product it need to select productSold that top1 from Row_Number\n",
    "df_favourite_product = spark.sql(\"\"\"\n",
    "SELECT custId,\n",
    "productSold,\n",
    "unitsSold\n",
    "FROM view_favourite_product_sumUnitsSold_DESC\n",
    "WHERE row_num = 1\n",
    "\"\"\")\n",
    "\n",
    "#Create view that show favourite_product\n",
    "df_favourite_product.createOrReplaceTempView('view_favourite_product')\n",
    "\n",
    "df_favourite_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+--------------+\n",
      "|customer_id|favourite_product|longest_streak|\n",
      "+-----------+-----------------+--------------+\n",
      "|      23938|          PURA250|           185|\n",
      "+-----------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step4 : Inner join customerID to find longest_streak and favourite_product from view longest_streak and favourite_product\n",
    "df_longest_streak_favourite_product = spark.sql(\"\"\"\n",
    "SELECT f.custId AS customer_id,\n",
    "f.productSold AS favourite_product,\n",
    "l.DateDiff_InDays AS longest_streak\n",
    "FROM view_longest_streak l\n",
    "INNER JOIN view_favourite_product f\n",
    "ON l.custId = f.custId\n",
    "\"\"\")\n",
    "\n",
    "df_longest_streak_favourite_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5 : Convert to Pandas Dataframe to Prepare send csv to output file \n",
    "pd_df_longest_streak_favourite_product = df_longest_streak_favourite_product.toPandas()\n",
    "pd_df_longest_streak_favourite_product.to_csv('result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nI have get stuck on the step to write data to Postgressql\\n        Explain: I have prepare to created table on PostgresSql on docker-compose file\\nif you run docker-compose you will found customer table on postgres database\\nbut I was stuck on this step to write data to Postgressql because error \"java.sql.SQLException: No suitable driver\"\\nso I try to find information either other user was suggest to download Spark JDBC then move it to jars location of Spark file\\nand it was unresolved so I have to changed the requirements and output to the csv that was attach in zip file\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For parts that would like to change requirements\n",
    "\"\"\"\n",
    "I have get stuck on the step to write data to Postgressql\n",
    "    Explain: I have prepare to created table on PostgresSql on docker-compose file\n",
    "if you run docker-compose you will found customers table on warehouse database and I separate problem that can't\n",
    "write data to Postgressql for 2 scenario\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# #Scenario1\n",
    "I was stuck on this step to write data to Postgressql because error \"java.sql.SQLException: No suitable driver\"\n",
    "I try to find information each other user was suggest to download Spark JDBC then move it to jars location of Spark file\n",
    "and I try to that way and it was unresolved so in this case I would like to changed the requirements and output to the csv \n",
    "format that was attached in a zip file\n",
    "\"\"\"\n",
    "\n",
    "# PSQL_SERVERNAME = \"postgres\"\n",
    "# PSQL_PORTNUMBER = 5432\n",
    "# PSQL_DBNAME = \"postgres\"\n",
    "# PSQL_USERNAME = \"admin\"\n",
    "# PSQL_PASSWORD = \"password\"\n",
    "# TABLE_CUSTOMER = \"customer\"\n",
    "\n",
    "# URL = f\"jdbc:postgresql://{PSQL_SERVERNAME}/{PSQL_DBNAME}\"\n",
    "# print(URL)\n",
    "\n",
    "# df_longest_streak_favourite_product.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\",URL) \\\n",
    "#     .option(\"dbtable\",TABLE_CUSTOMER) \\\n",
    "#     .option(\"user\",PSQL_USERNAME) \\\n",
    "#     .option(\"password\",PSQL_PASSWORD) \\\n",
    "#     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# #Scenario2\n",
    "I try to use psycopg2 and sqlalchemy lib.I found this problem that have issue from can't connect to Postgressql \n",
    "and it was showed \"invaild password\".Although try input correctly value from docker-compose\n",
    "\"\"\"\n",
    "\n",
    "# import psycopg2\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# # Create Engine\n",
    "# engine = create_engine(\"postgresql+psycopg2://postgres:password@localhost:5432/warehouse?client_encoding=utf8\")\n",
    "# # Save result to the database via engine\n",
    "# pd_df_longest_streak_favourite_product.to_sql('customers', engine, index=False, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
